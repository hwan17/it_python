{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T01:01:48.655895Z",
     "start_time": "2020-05-28T01:01:48.033279Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def j_scroll():\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    list_url = \"https://news.joins.com/Search/JoongangNews?page=1&Keyword=%EC%BD%94%EB%A1%9C%EB%82%98&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "    url = urllib.request.Request(list_url)\n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    result2 = soup.find_all('h2', class_='headline mg')\n",
    "    params=[]\n",
    "    for i in result2:\n",
    "        for k in i:\n",
    "            params.append(k.get(\"href\"))\n",
    "        #print(i.get_text())\n",
    "    return params\n",
    "#print(ebs_scroll() )\n",
    "\n",
    "list_url=j_scroll()\n",
    "print(list_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T01:27:59.961246Z",
     "start_time": "2020-05-28T01:27:51.769073Z"
    }
   },
   "outputs": [],
   "source": [
    "def j_detail_scroll():\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    f = open('c:\\\\data\\\\mytext20.txt','w',encoding = \"UTF-8\")\n",
    "\n",
    "    #list_url = \"https://news.joins.com/article/23787116\"\n",
    "    list_url=j_scroll()\n",
    "    for j in list_url:\n",
    "        url = urllib.request.Request(j)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('div', id=\"article_body\")\n",
    "        #params=[]\n",
    "        for i in result2:\n",
    "            #print(i.get_text())\n",
    "            f.writelines(i.get_text()+'\\n')\n",
    "    \n",
    "    f.close()\n",
    "#print(ebs_scroll() )\n",
    "#j_scroll()\n",
    "j_detail_scroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T01:48:07.898683Z",
     "start_time": "2020-05-28T01:47:59.512467Z"
    }
   },
   "outputs": [],
   "source": [
    "def j_detail_scroll():\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    f = open('c:\\\\data\\\\mytext21.txt','w',encoding = \"UTF-8\")\n",
    "#    params=[]\n",
    "\n",
    "    #list_url = \"https://news.joins.com/article/23787116\"\n",
    "    list_url=j_scroll()\n",
    "    for j in list_url:\n",
    "        url = urllib.request.Request(j)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('div', id=\"article_body\")\n",
    "        for i in result2:\n",
    "            #print(i.get_text())\n",
    "            f.write(str(i.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "    #f.write(str(params))\n",
    "    f.close()\n",
    "#print(ebs_scroll() )\n",
    "#j_scroll()\n",
    "j_detail_scroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://news.joins.com/Search/JoongangNews?page=1&Keyword=%EC%BD%94%EB%A1%9C%EB%82%98&SortType=New&SearchCategoryType=JoongangNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T05:35:11.485413Z",
     "start_time": "2020-05-28T05:35:07.183166Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def j_scroll():\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    params=[]\n",
    "    for k in range(1,4):\n",
    "        list_url = \"https://news.joins.com/Search/JoongangNews?page=\"+str(k)+\"&Keyword=%EC%BD%94%EB%A1%9C%EB%82%98&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "        url = urllib.request.Request(list_url)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('h2', class_='headline mg')\n",
    "        for i in result2:\n",
    "            for k in i:\n",
    "                #params.append(k.get(\"href\"))\n",
    "                params.append(k)\n",
    "            #print(i.get_text())\n",
    "    return params\n",
    "#print(ebs_scroll() )\n",
    "\n",
    "list_url=j_scroll()\n",
    "print(list_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T01:53:37.521275Z",
     "start_time": "2020-05-28T01:53:14.478321Z"
    }
   },
   "outputs": [],
   "source": [
    "def j_detail_scroll():\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    f = open('c:\\\\data\\\\mytext22.txt','w',encoding = \"UTF-8\")\n",
    "#    params=[]\n",
    "\n",
    "    #list_url = \"https://news.joins.com/article/23787116\"\n",
    "    list_url=j_scroll()\n",
    "    for j in list_url:\n",
    "        url = urllib.request.Request(j)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('div', id=\"article_body\")\n",
    "        for i in result2:\n",
    "            #print(i.get_text())\n",
    "            f.write(str(i.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "    #f.write(str(params))\n",
    "    f.close()\n",
    "#print(ebs_scroll() )\n",
    "#j_scroll()\n",
    "j_detail_scroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T02:37:40.724368Z",
     "start_time": "2020-05-28T02:37:15.700866Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt  # 그래프 그리는 모듈\n",
    "from os import path     #  os 에 있는 파일을 파이썬에서 인식하기 위해서\n",
    "import re   #  데이터 정제를 위해서 필요한 모듈 \n",
    "import numpy as np  \n",
    "from PIL import Image  # 이미지 시각화를 위한 모듈 \n",
    "\n",
    "# 워드 클라우드의 배경이 되는 이미지 모양을 결정\n",
    "usa_mask = np.array(Image.open(\"c:/project/s_korea.png\"))\n",
    "\n",
    "# 워드 클라우드를 그릴 스크립트 이름을 물어봅니다. \n",
    "script = input( 'input file name : ')\n",
    "title = input( '저장 이름 : ')\n",
    "# 워드 클라우드 그림이 저장될 작업 디렉토리를 설정 \n",
    "d = path.dirname(\"c:/project/\")\n",
    "\n",
    "# 기사 스크립트와 os 의 위치를 연결하여 utf8로 인코딩해서 한글 텍스트를\n",
    "# text 변수로 리턴한다.\n",
    "text = open(path.join(d, \"%s\"%script), mode=\"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "# 파이썬이 인식할 수 있는 한글 단어의 갯수를 늘리기 위한 작업 \n",
    "file = open('c:/project/word.txt', 'r', encoding = 'utf-8')\n",
    "word = file.read().split(' ')\n",
    "for i in word:\n",
    "    text = re.sub(i,'',text)\n",
    "\n",
    "# 워드 클라우드를 그린다. \n",
    "wordcloud = WordCloud(font_path='C://Windows//Fonts//gulim', # 글씨체\n",
    "                      stopwords=STOPWORDS,   # 마침표, 느낌표,싱글 쿼테이션 등을 정제\n",
    "                      max_words=1000, # 워드 클라우드에 그릴 최대 단어갯수\n",
    "                      background_color='white', # 배경색깔\n",
    "                      max_font_size = 100, # 최대 글씨 크기 \n",
    "                      min_font_size = 1, # 최소 글씨 \n",
    "                      mask = usa_mask, # 배경 모양 \n",
    "                      colormap='jet').generate(text).to_file('c:/project/'+title +'.png')\n",
    "                  # c 드라이브 밑에 project 폴더 밑에 생성되는 워드 클라우드 이미지 이름\n",
    "  \n",
    "plt.figure(figsize=(15,15)) \n",
    "plt.imshow(wordcloud, interpolation='bilinear')  # 글씨가 퍼지는 스타일 \n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#text.close()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T02:52:43.334964Z",
     "start_time": "2020-05-28T02:52:40.235734Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def j_scroll(topic, page):\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib import parse\n",
    "    #topic = input('')\n",
    "    asciitopic = parse.quote(topic)\n",
    "    params=[]\n",
    "    for k in range(1,page+1):\n",
    "        list_url = \"https://news.joins.com/Search/JoongangNews?page=\"+str(k)+\"&Keyword=\"+asciitopic+\"&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "        url = urllib.request.Request(list_url)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('h2', class_='headline mg')\n",
    "        for i in result2:\n",
    "            for k in i:\n",
    "                params.append(k.get(\"href\"))\n",
    "            #print(i.get_text())\n",
    "    return params\n",
    "#print(ebs_scroll() )\n",
    "\n",
    "list_url=j_scroll('중이병',3)\n",
    "print(list_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T02:57:41.647490Z",
     "start_time": "2020-05-28T02:57:41.634500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from my_loc import j_article\n",
    "\n",
    "j_detail_scroll('중이병',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T05:12:30.663261Z",
     "start_time": "2020-05-28T05:12:23.539884Z"
    }
   },
   "outputs": [],
   "source": [
    "def jscroll(topic,dirr, page):\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib import parse\n",
    "    #topic = input('')\n",
    "    asciitopic = parse.quote(topic)\n",
    "    params=[]\n",
    "    for k in range(1,page+1):\n",
    "        list_url = \"https://news.joins.com/Search/JoongangNews?page=\"+str(k)+\"&Keyword=\"+asciitopic+\"&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "        url = urllib.request.Request(list_url)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('h2', class_='headline mg')\n",
    "        for i in result2:\n",
    "            for k in i:\n",
    "                params.append(k.get(\"href\"))\n",
    "            #print(i.get_text())\n",
    "#    return params\n",
    "#print(ebs_scroll() )\n",
    "    f = open(dirr,'w',encoding = \"UTF-8\")\n",
    "#    params=[]\n",
    "\n",
    "    #list_url = \"https://news.joins.com/article/23787116\"\n",
    "    list_url=params\n",
    "    for j in list_url:\n",
    "        url = urllib.request.Request(j)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result1 = soup.find_all('h1', id=\"article_title\")\n",
    "        result2 = soup.find_all('div', id=\"article_body\")\n",
    "        for j in result1:\n",
    "            f.write('기사제목 : '+str(j.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "            for i in result2:\n",
    "                #print(i.get_text())\n",
    "                f.write(str(i.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "        #f.write(str(params))\n",
    "    f.close()\n",
    "#print(ebs_scroll() )\n",
    "#j_scroll()\n",
    "#j_detail_scroll()\n",
    "\n",
    "jscroll('코로나','c:\\\\data\\\\mytext24.txt',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T03:03:52.734409Z",
     "start_time": "2020-05-28T03:03:28.983016Z"
    }
   },
   "outputs": [],
   "source": [
    "jscroll('박지성','c:\\\\data\\\\mytext23.txt',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T04:28:51.294314Z",
     "start_time": "2020-05-28T04:28:51.289316Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'c:\\\\data\\\\mytext23.txt'.index('\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T05:03:43.783844Z",
     "start_time": "2020-05-28T05:03:43.439056Z"
    }
   },
   "outputs": [],
   "source": [
    "def chu_title():\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    params=[]\n",
    "    for k in range(1,2):\n",
    "        list_url = \"https://news.joins.com/Search/JoongangNews?page=\"+str(k)+\"&Keyword=%EC%BD%94%EB%A1%9C%EB%82%98&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "        url = urllib.request.Request(list_url)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('h2', class_='headline mg')\n",
    "        for i in result2:\n",
    "            for k in i:\n",
    "                params.append(k.get_text())\n",
    "            #print(i.get_text())\n",
    "    return params\n",
    "\n",
    "chu_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def j_scroll():\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    params=[]\n",
    "    for k in range(1,4):\n",
    "        list_url = \"https://news.joins.com/Search/JoongangNews?page=\"+str(k)+\"&Keyword=%EC%BD%94%EB%A1%9C%EB%82%98&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "        url = urllib.request.Request(list_url)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('h2', class_='headline mg')\n",
    "        for i in result2:\n",
    "            for k in i:\n",
    "                params.append(k.get(\"href\"))\n",
    "            #print(i.get_text())\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T05:26:09.273456Z",
     "start_time": "2020-05-28T05:26:08.700565Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dscroll(topic,dirr, page):\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib import parse\n",
    "    #topic = input('')\n",
    "    asciitopic = parse.quote(topic)\n",
    "    params=[]\n",
    "    for k in range(1,page+1):\n",
    "        list_url = \"https://www.donga.com/news/search?p=\"+str(1+15*(k-1))+\"&query=\"+asciitopic+\"&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\"\n",
    "        url = urllib.request.Request(list_url)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('a')\n",
    "        for i in result2:\n",
    "            for k in i:\n",
    "                params.append(k.get(\"href\"))\n",
    "            #print(i.get_text())\n",
    "#    return params\n",
    "    print(params)\n",
    "#print(ebs_scroll() )\n",
    "    f = open(dirr,'w',encoding = \"UTF-8\")\n",
    "#    params=[]\n",
    "\n",
    "    #list_url = \"https://news.joins.com/article/23787116\"\n",
    "    list_url=params\n",
    "    for j in list_url:\n",
    "        url = urllib.request.Request(j)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result1 = soup.find_all('h1', id=\"title\")\n",
    "        result2 = soup.find_all('div', id=\"article_txt\")\n",
    "        for j in result1:\n",
    "            f.write('기사제목 : '+str(j.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "            for i in result2:\n",
    "                #print(i.get_text())\n",
    "                f.write(str(i.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "        #f.write(str(params))\n",
    "    f.close()\n",
    "#print(ebs_scroll() )\n",
    "#j_scroll()\n",
    "#j_detail_scroll()\n",
    "\n",
    "dscroll('인공지능','c:\\\\data\\\\mytext25.txt',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T05:49:52.664465Z",
     "start_time": "2020-05-28T05:49:49.433745Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dscroll2(topic,dirr,page):\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib import parse\n",
    "    #topic = input('')\n",
    "    asciitopic = parse.quote(topic)\n",
    "    params=[]\n",
    "    for k in range(1,page+1):\n",
    "        list_url = \"https://www.donga.com/news/search?p=\"+str(1+15*(k-1))+\"&query=\"+asciitopic+\"&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\"\n",
    "        url = urllib.request.Request(list_url)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('p', class_='txt')\n",
    "        for i in result2:\n",
    "            for j in i:\n",
    "                #for g in j:\n",
    "                params.append(j.get('href'))\n",
    "                #params.append(k)\n",
    "            #print(i.get_text())\n",
    "#    return params\n",
    "    print(params)\n",
    "    \n",
    "    f = open(dirr,'w',encoding = \"UTF-8\")\n",
    "#    params=[]\n",
    "\n",
    "    #list_url = \"https://news.joins.com/article/23787116\"\n",
    "    list_url=params\n",
    "    for j in list_url:\n",
    "        url = urllib.request.Request(j)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result1 = soup.find_all('h1', class_=\"title\")\n",
    "        result2 = soup.find_all('div', class_=\"article_txt\")\n",
    "        for j in result1:\n",
    "            f.write('기사제목 : '+str(j.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "            for i in result2:\n",
    "                #print(i.get_text())\n",
    "                f.write(str(i.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "        #f.write(str(params))\n",
    "    f.close()\n",
    "#print(ebs_scroll() )\n",
    "#j_scroll()\n",
    "#j_detail_scroll()\n",
    "\n",
    "#dscroll('인공지능','c:\\\\data\\\\mytext25.txt',1)\n",
    "    \n",
    "dscroll2('인공지능','c:\\\\data\\\\mytext25.txt',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T05:37:18.599767Z",
     "start_time": "2020-05-28T05:37:18.126669Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def j_scroll():\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    params=[]\n",
    "    for k in range(1,2):\n",
    "        list_url = \"https://news.joins.com/Search/JoongangNews?page=\"+str(k)+\"&Keyword=%EC%BD%94%EB%A1%9C%EB%82%98&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "        url = urllib.request.Request(list_url)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result2 = soup.find_all('h2', class_='headline mg')\n",
    "        for i in result2:\n",
    "            for k in i:\n",
    "                params.append(k.get(\"href\"))\n",
    "                #params.append(k)\n",
    "            #print(i.get_text())\n",
    "    return params\n",
    "#print(ebs_scroll() )\n",
    "\n",
    "list_url=j_scroll()\n",
    "print(list_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T06:06:43.743761Z",
     "start_time": "2020-05-28T06:06:40.377504Z"
    }
   },
   "outputs": [],
   "source": [
    "def dscroll3(topic,dirr,page):\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib import parse\n",
    "    #topic = input('')\n",
    "    asciitopic = parse.quote(topic)\n",
    "    params=[]\n",
    "    for k in range(1,page+1):\n",
    "        list_url = \"https://www.donga.com/news/search?p=\"+str(1+15*(k-1))+\"&query=\"+asciitopic+\"&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\"\n",
    "        url = urllib.request.Request(list_url)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"lxml\")\n",
    "        result2 = soup.find_all('p', class_='txt')\n",
    "        for i in result2:\n",
    "            #for j in i:\n",
    "                #for g in j:\n",
    "            params.append(i.get('href'))\n",
    "                #params.append(k)\n",
    "            #print(i.get_text())\n",
    "#    return params\n",
    "    print(params)\n",
    "    \n",
    "    f = open(dirr,'w',encoding = \"UTF-8\")\n",
    "#    params=[]\n",
    "\n",
    "    #list_url = \"https://news.joins.com/article/23787116\"\n",
    "    list_url=params\n",
    "    for j in list_url:\n",
    "        url = urllib.request.Request(j)\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        result1 = soup.find_all('h1', class_=\"title\")\n",
    "        result2 = soup.find_all('div', class_=\"article_txt\")\n",
    "        for j in result1:\n",
    "            f.write('기사제목 : '+str(j.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "            for i in result2:\n",
    "                #print(i.get_text())\n",
    "                f.write(str(i.get_text(\" \",strip=True))+'\\n'+'\\n')\n",
    "        #f.write(str(params))\n",
    "    f.close()\n",
    "#print(ebs_scroll() )\n",
    "#j_scroll()\n",
    "#j_detail_scroll()\n",
    "\n",
    "#dscroll('인공지능','c:\\\\data\\\\mytext25.txt',1)\n",
    "    \n",
    "dscroll2('인공지능','c:\\\\data\\\\mytext26.txt',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T06:59:20.185577Z",
     "start_time": "2020-05-28T06:56:40.138647Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "######################################################\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "from  bs4 import BeautifulSoup\n",
    "from selenium import webdriver  # 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time                     # 중간중간 sleep 을 걸어야 해서 time 모듈 import\n",
    "\n",
    "########################### url 받아오기 ###########################\n",
    "\n",
    "# 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 위의 위치에 둔다\n",
    "# 팬텀 js로 하면 백그라운드로 실행할 수 있음\n",
    "binary = 'C:\\chromedriver/chromedriver.exe'\n",
    "\n",
    "# 브라우져를 인스턴스화\n",
    "browser = webdriver.Chrome(binary)\n",
    "\n",
    "# 구글의 이미지 검색 url 받아옴(아무것도 안 쳤을때의 url)\n",
    "browser.get(\"https://www.google.co.kr/imghp?hl=ko&tab=wi&ei=l1AdWbegOcra8QXvtr-4Cw&ved=0EKouCBUoAQ\")\n",
    "\n",
    "# 구글의 이미지 검색에 해당하는 input 창의 id 가 '  ?  ' 임(검색창에 해당하는 html코드를 찾아서 elem 사용하도록 설정)\n",
    "# input창 찾는 방법은 원노트에 있음\n",
    "\n",
    "#elem = browser.find_elements_by_class_name('gLFyf gsfi')\n",
    "\n",
    "elem = browser.find_element_by_xpath(\"//*[@class='gLFyf gsfi']\") \n",
    "\n",
    "\n",
    "\n",
    "########################### 검색어 입력 ###########################\n",
    "\n",
    "# elem 이 input 창과 연결되어 스스로 햄버거를 검색\n",
    "elem.send_keys(\"비니쿤카\")\n",
    "\n",
    "# 웹에서의 submit 은 엔터의 역할을 함\n",
    "elem.submit()\n",
    "\n",
    "########################### 반복할 횟수 ###########################\n",
    "\n",
    "# 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함\n",
    "for i in range(1, 5):\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)                  # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "\n",
    "########################### 그림파일 저장 ###########################\n",
    "\n",
    "\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    imgList = soup.find_all(\"img\", class_=\"rg_i\")  # 구글 이미지 url 이 있는 img 태그의 _img 클래스에 가서\n",
    "    for im in imgList:\n",
    "        try :\n",
    "            params.append(im[\"src\"])                   # params 리스트에 image url 을 담음\n",
    "        except KeyError:\n",
    "            params.append(im[\"data-src\"])\n",
    "    return params\n",
    "\n",
    "# 이미지의 상세 url 의 값이 있는 src 가 없을 경우\n",
    "# data-src 로 가져오시오 ~ \n",
    "\n",
    "\n",
    "def fetch_detail_url():\n",
    "    params = fetch_list_url()\n",
    "\n",
    "    for idx,p in enumerate(params,1):\n",
    "        # 다운받을 폴더경로 입력\n",
    "        urllib.request.urlretrieve(p, \"C:/googleimages/\" + str(idx) + \".jpg\")\n",
    "\n",
    "# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\n",
    "# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\n",
    "\n",
    "fetch_detail_url()\n",
    "\n",
    "# 끝나면 브라우져 닫기\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T00:22:29.525856Z",
     "start_time": "2020-05-29T00:21:26.508510Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "######################################################\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "from  bs4 import BeautifulSoup\n",
    "from selenium import webdriver  # 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time                     # 중간중간 sleep 을 걸어야 해서 time 모듈 import\n",
    "\n",
    "########################### url 받아오기 ###########################\n",
    "\n",
    "# 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 위의 위치에 둔다\n",
    "# 팬텀 js로 하면 백그라운드로 실행할 수 있음\n",
    "binary = 'C:\\chromedriver/chromedriver.exe'\n",
    "\n",
    "# 브라우져를 인스턴스화\n",
    "browser = webdriver.Chrome(binary)\n",
    "\n",
    "# 구글의 이미지 검색 url 받아옴(아무것도 안 쳤을때의 url)\n",
    "browser.get(\"https://www.bing.com/?scope=images&nr=1&FORM=NOFORM\")\n",
    "\n",
    "# 구글의 이미지 검색에 해당하는 input 창의 id 가 '  ?  ' 임(검색창에 해당하는 html코드를 찾아서 elem 사용하도록 설정)\n",
    "# input창 찾는 방법은 원노트에 있음\n",
    "\n",
    "#elem = browser.find_elements_by_class_name('gLFyf gsfi')\n",
    "\n",
    "elem = browser.find_element_by_id(\"sb_form_q\") \n",
    "\n",
    "\n",
    "\n",
    "########################### 검색어 입력 ###########################\n",
    "\n",
    "# elem 이 input 창과 연결되어 스스로 햄버거를 검색\n",
    "elem.send_keys(\"조커\")\n",
    "\n",
    "# 웹에서의 submit 은 엔터의 역할을 함\n",
    "elem.submit()\n",
    "\n",
    "########################### 반복할 횟수 ###########################\n",
    "\n",
    "# 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함\n",
    "for i in range(1, 5):\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)                  # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "\n",
    "########################### 그림파일 저장 ###########################\n",
    "\n",
    "\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    imgList = soup.find_all(\"img\", class_=\"mimg\")  # 구글 이미지 url 이 있는 img 태그의 _img 클래스에 가서\n",
    "    for im in imgList:\n",
    "        try :\n",
    "            params.append(im[\"src\"])                   # params 리스트에 image url 을 담음\n",
    "        except KeyError:\n",
    "            params.append(im[\"data-src\"])\n",
    "    return params\n",
    "\n",
    "# 이미지의 상세 url 의 값이 있는 src 가 없을 경우\n",
    "# data-src 로 가져오시오 ~ \n",
    "\n",
    "\n",
    "def fetch_detail_url():\n",
    "    params = fetch_list_url()\n",
    "\n",
    "    for idx,p in enumerate(params,1):\n",
    "        # 다운받을 폴더경로 입력\n",
    "        urllib.request.urlretrieve(p, \"C:/bing/\" + str(idx) + \".jpg\")\n",
    "\n",
    "# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\n",
    "# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\n",
    "\n",
    "fetch_detail_url()\n",
    "\n",
    "# 끝나면 브라우져 닫기\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T02:02:47.913847Z",
     "start_time": "2020-05-29T02:02:47.901855Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec 11 08:34:57 2019\n",
    "\n",
    "@author: wdp\n",
    "\"\"\"\n",
    "from pytube import YouTube\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#workdir = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "\n",
    "\n",
    "workdir = os.path.dirname(os.path.abspath(sys.argv[0]))\n",
    "\n",
    "url = input('playlist url : ').rstrip()\n",
    "\n",
    "\n",
    "\n",
    "if 'watch' in url:\n",
    "\n",
    "    url = 'https://youtube.com/playlist?list=' + url.split('list=')[-1].split('&')[0]\n",
    "\n",
    "\n",
    "\n",
    "res = requests.get(url)\n",
    "\n",
    "source = res.text #Response의 바디를 source라는 변수에 저장합니다. 이는 Raw Text 입니다.\n",
    "\n",
    "vid_ids = [x.split('href=\"/watch?v=')[1].split('&amp;')[0] for x in source.split('\\n') if 'pl-video-title-link' in x]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for vid in vid_ids:\n",
    "\n",
    "    getStr = 'https://www.youtube.com/watch?v=' + vid\n",
    "\n",
    "    yt = YouTube(getStr)\n",
    "\n",
    "    file_name = yt.title\n",
    "\n",
    "    print('Downloading %s' % (file_name))\n",
    "\n",
    "\n",
    "\n",
    "    yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download('C:\\\\data') # 파일\n",
    "    #file_extension은 재생목록 파일들 중에서 mp4만 추출해서 다운로드 받겠다 는뜻\n",
    "    # progressive는 720p까지만 지원 가능한 다운로드 방식\n",
    "\n",
    "    output = ''\n",
    "\n",
    "\n",
    "    print('Completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T02:11:57.997546Z",
     "start_time": "2020-05-29T02:11:57.986553Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-7-9b88c0b7f514>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-9b88c0b7f514>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    url = 'https://youtube.com/playlist?list=' + url.split('list=')[-1].split('&')[0]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec 11 08:34:57 2019\n",
    "\n",
    "@author: wdp\n",
    "\"\"\"\n",
    "from pytube import YouTube\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#workdir = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "workdir = os.path.dirname(os.path.abspath(sys.argv[0]))\n",
    "url = input('playlist url : ').rstrip()\n",
    "\n",
    "if 'watch' in url:\n",
    "    url = 'https://youtube.com/playlist?list=' + url.split('list=')[-1].split('&')[0]\n",
    "\n",
    "res = requests.get(url)\n",
    "source = res.text #Response의 바디를 source라는 변수에 저장합니다. 이는 Raw Text 입니다.\n",
    "vid_ids = [x.split('href=\"/watch?v=')[1].split('&amp;')[0] for x in source.split('\\n') if 'pl-video-title-link' in x]\n",
    "\n",
    "for vid in vid_ids:\n",
    "    getStr = 'https://www.youtube.com/watch?v=' + vid\n",
    "    yt = YouTube(getStr)\n",
    "    file_name = yt.title\n",
    "    print('Downloading %s' % (file_name))\n",
    "\n",
    "    yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download('C:\\\\data') # 파일\n",
    "    #file_extension은 재생목록 파일들 중에서 mp4만 추출해서 다운로드 받겠다 는뜻\n",
    "    # progressive는 720p까지만 지원 가능한 다운로드 방식\n",
    "    output = ''\n",
    "    print('Completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
